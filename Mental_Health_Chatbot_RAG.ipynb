{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install libraries"
      ],
      "metadata": {
        "id": "8uf-87O5iowl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FnUEF_lHDaaC"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet fsspec==2025.3.2 huggingface_hub sentence-transformers transformers datasets hf_xet\n",
        "%pip install --upgrade --quiet  langchain langchain-community langchain-huggingface langchain-chroma\n",
        "%pip install beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Secret keys"
      ],
      "metadata": {
        "id": "vLB39EITi8sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "LANGSMITH_API_KEY = userdata.get('LANGSMITH_API_KEY')"
      ],
      "metadata": {
        "id": "1SPrWOrzIH9q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process pdf documents"
      ],
      "metadata": {
        "id": "g1QBKOA_jEh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload pdf\n",
        "\n",
        "%pip install PyPDF2\n",
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file in binary mode\n",
        "with open('manage-stress-workbook.pdf', 'rb') as file:\n",
        "    # Create a PdfFileReader object\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    pdf_texts = [p.extract_text().strip() for p in pdf_reader.pages]\n",
        "\n",
        "    # Filter the empty strings\n",
        "    pdf_texts = [text for text in pdf_texts if text]\n",
        "\n",
        "    print(pdf_texts[0])"
      ],
      "metadata": {
        "id": "71h8PuvBfKjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chunk pdf\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter, CharacterTextSplitter\n",
        "\n",
        "character_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\"],\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=10\n",
        ")\n",
        "docs = character_splitter.create_documents(pdf_texts)\n",
        "\n",
        "print(docs[12])\n",
        "\n",
        "print(f\"Number of document chunks: {len(docs)}\")\n",
        "print(f\"First chunk: {docs[0].page_content}\")"
      ],
      "metadata": {
        "id": "-YcYf83KGhY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process information from the web"
      ],
      "metadata": {
        "id": "qAzUJQrpjeT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: load website content\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def load_website_content(urls):\n",
        "    loader = WebBaseLoader(urls)\n",
        "    documents = loader.load()\n",
        "    print(f\"Loaded content from {len(urls)} URLs\")\n",
        "    return documents\n"
      ],
      "metadata": {
        "id": "aBxjfT5p51xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Clean and chunk the content\n",
        "\n",
        "\n",
        "def process_web_documents(documents):\n",
        "    # HTML cleaning is handled by WebBaseLoader, but you can add additional processing\n",
        "    character_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=50,\n",
        "        chunk_overlap=10\n",
        "    )\n",
        "    # Extract text content from Document objects\n",
        "    texts = [doc.page_content for doc in documents]\n",
        "\n",
        "    chunks = character_splitter.create_documents(texts)\n",
        "    print(f\"Split into {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "# Example usage\n",
        "urls = [\n",
        "    \"https://www.rcpsych.ac.uk/mental-health/mental-illnesses-and-mental-health-problems\",\n",
        "    \"https://www.psychiatry.org/patients-families\"\n",
        "]\n",
        "\n",
        "web_documents = load_website_content(urls)\n",
        "chunks = process_web_documents(web_documents)"
      ],
      "metadata": {
        "id": "-wDolw-O6m5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings using HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "9D4lEVL7juob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create embeddings with hugging face sentence transformer\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "# Choose a suitable sentence transformer model from Hugging Face Hub\n",
        "# Recommended models: 'all-mpnet-base-v2', 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n"
      ],
      "metadata": {
        "id": "h6QXM2tTDrcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create vector store\n",
        "\n",
        "### Chromadb used. It automatically indexes the embeddings"
      ],
      "metadata": {
        "id": "hahoc8Mcj4mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create chromadb vector store and index the embeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "persist_directory=\"chroma_db\"\n",
        "\n",
        "#vector_store = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
        "all_documents = docs + chunks\n",
        "\n",
        "vector_store = Chroma.from_documents(documents=all_documents,\n",
        "                                     embedding=embeddings,\n",
        "                                     persist_directory=\"web_and_pdf_db\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Hgi_whTLaZ1v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt creation using a template\n",
        "\n",
        "### Templates enable creation of longer and with well described context"
      ],
      "metadata": {
        "id": "mEJsBBzAkJCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are an experienced mental health counselor \\n\n",
        "providing support and information about mental health by web chat, to a service user. \\n\n",
        "Use ONLY the following information to answer the user's question at the end. \\n\n",
        "You need to make sure that you provide the most accurate information and best support! \\n\n",
        "Provide concise, accurate and as complete an answer as possible. \\n\n",
        "Do not make any assumptions or make up an answer. \\n\n",
        "If the information does not contain the answer, please respond with: \\n\n",
        "\"Based on the information provided, I cannot answer your question.\" \\n\\n\n",
        "\n",
        "\n",
        "Information: {context}\n",
        "\n",
        "\n",
        "Question: {question} \\n\\n\n",
        "Answer:\"\"\""
      ],
      "metadata": {
        "id": "SMSC592G3uwT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "Xkl1nyyi6eiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing, HuggingFace pipeline and information retrieval mechanism"
      ],
      "metadata": {
        "id": "3R4vu7x9koqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the retrieval mechanism\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "\n",
        "# Choose a Hugging Face language model for generation\n",
        "model_name = \"gpt2-large\"      # You can try other models\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a Hugging Face pipeline for text generation\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=200,\n",
        "                temperature=0.1,\n",
        "                top_p=0.1,\n",
        "                repetition_penalty=1.15)\n",
        "\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Initialize the RetrievalQA chain with the ChromaDB retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "rag_chain = RetrievalQA.from_llm(llm=llm, retriever=retriever, prompt=prompt)"
      ],
      "metadata": {
        "id": "MVKZjOaWEur-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test your model"
      ],
      "metadata": {
        "id": "4FaTzZHvkzDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ask a question to perform an embeddings search on the chroma db database\n",
        "\n",
        "query = \"What are the symptoms of depression?\"\n",
        "\n",
        "result = rag_chain({\"query\": query})\n",
        "print(result[\"result\"])"
      ],
      "metadata": {
        "id": "KWweBiEdXAlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Using bullet points, list five examples of mindfulness based stress reduction techniques.\"\n",
        "\n",
        "result = rag_chain({\"query\": query})\n",
        "print(result[\"result\"])"
      ],
      "metadata": {
        "id": "tjn1crmCwx-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}